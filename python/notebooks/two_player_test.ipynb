{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Experimenting with having two actors/players whoose states and actions affect each other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing relevant libraries.\n",
    "import numpy as np\n",
    "from itertools import product\n",
    "import pandas as pd\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global settings and setup of game.\n",
    "n_players = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking that no probabilities are negative.\n",
    "def mkSimpleProb(pairs: list[tuple[str, float]]) -> dict[str, float]:\n",
    "    dist: dict[str, float] = {}\n",
    "    for (st, pr) in pairs:\n",
    "        if pr >= 0:\n",
    "            dist[st] = pr\n",
    "    return dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Player:\n",
    "    def __init__(self, name: str, probs: dict[str, float], states: list[str], actions):\n",
    "        self.name = name\n",
    "        self.pC_Confess = probs[\"pC_Confess\"]\n",
    "        self.pR_Confess = probs[\"pR_Confess\"]\n",
    "        self.pC_Refuse = probs[\"pC_Refuse\"]\n",
    "        self.pR_Refuse = probs[\"pR_Refuse\"]\n",
    "        self.actions = actions\n",
    "        self.last_action = None\n",
    "        self.score = 0.0\n",
    "\n",
    "    def choose_action(self, x:str) -> str:\n",
    "        \"\"\"For now, an action is chosen at random.\"\"\"\n",
    "        action = np.random.choice(self.actions(x))\n",
    "        self.last_action = action\n",
    "        return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Game:\n",
    "    def __init__(self, players: list[Player], states: list[str]):\n",
    "        self.players = players\n",
    "        self.states = states\n",
    "        self.current_state = \"Start\"\n",
    "        self.zero = 0.0 # Default value of zero-length policy sequences.\n",
    "\n",
    "    def nextFunc(self, t: int, x: str, ys: dict[str, str]) -> dict[str, float]:\n",
    "        a1 = ys[self.players[0].name]\n",
    "        a2 = ys[self.players[1].name]\n",
    "\n",
    "        if a1 == \"Confess\" and a2 == \"Confess\":\n",
    "            return mkSimpleProb([\n",
    "                (\"CC\", self.players[0].pC_Confess * self.players[1].pC_Confess), #np.prod(player.pC_Confess for player in self.players)), something like this if arbitrary number of players. \n",
    "                (\"CR\", self.players[0].pC_Confess * self.players[1].pR_Confess),\n",
    "                (\"RC\", self.players[0].pR_Confess * self.players[1].pC_Confess),\n",
    "                (\"RR\", self.players[0].pR_Confess * self.players[1].pR_Confess)\n",
    "            ])\n",
    "        elif a1 == \"Confess\" and a2 == \"Refuse\":\n",
    "            return mkSimpleProb([\n",
    "                (\"CC\", self.players[0].pC_Confess * self.players[1].pC_Refuse),\n",
    "                (\"CR\", self.players[0].pC_Confess * self.players[1].pR_Refuse),\n",
    "                (\"RC\", self.players[0].pR_Confess * self.players[1].pC_Refuse),\n",
    "                (\"RR\", self.players[0].pR_Confess * self.players[1].pR_Refuse)\n",
    "            ])\n",
    "        elif a1 == \"Refuse\" and a2 == \"Confess\":\n",
    "            return mkSimpleProb([\n",
    "                (\"CC\", self.players[0].pC_Refuse * self.players[1].pC_Confess),\n",
    "                (\"CR\", self.players[0].pC_Refuse * self.players[1].pR_Confess),\n",
    "                (\"RC\", self.players[0].pR_Refuse * self.players[1].pC_Confess),\n",
    "                (\"RR\", self.players[0].pR_Refuse * self.players[1].pR_Confess)\n",
    "            ])\n",
    "        elif a1 == \"Refuse\" and a2 == \"Refuse\":\n",
    "            return mkSimpleProb([\n",
    "                (\"CC\", self.players[0].pC_Refuse * self.players[1].pC_Refuse),\n",
    "                (\"CR\", self.players[0].pC_Refuse * self.players[1].pR_Refuse),\n",
    "                (\"RC\", self.players[0].pR_Refuse * self.players[1].pC_Refuse),\n",
    "                (\"RR\", self.players[0].pR_Refuse * self.players[1].pR_Refuse)\n",
    "            ])\n",
    "        else:\n",
    "            raise ValueError(\"Invalid action combination.\")\n",
    "    \n",
    "    def reward(self, t: str, x: str, ys: dict[str, str], next_x: str) -> dict[str, float]:        \n",
    "        rewards = {}\n",
    "        if next_x == \"CC\":\n",
    "            rewards[self.players[0].name] = 1\n",
    "            rewards[self.players[1].name] = 1\n",
    "        elif next_x == \"CR\":\n",
    "            rewards[self.players[0].name] = 5\n",
    "            rewards[self.players[1].name] = 2\n",
    "        elif next_x == \"RC\":\n",
    "            rewards[self.players[0].name] = 2\n",
    "            rewards[self.players[1].name] = 5\n",
    "        elif next_x == \"RR\":\n",
    "            rewards[self.players[0].name] = 3\n",
    "            rewards[self.players[1].name] = 3\n",
    "        else:\n",
    "            raise ValueError(\"Invalid next state.\")\n",
    "        return rewards\n",
    "    \n",
    "    # Function defining how to add rewards together.\n",
    "    def add(self, a: float, b: float) -> dict[str, float]:\n",
    "        if type(a) != dict or type(b) != dict:\n",
    "            raise TypeError(f\"Inputs must be of type 'dict', not '{type(a).__name__}' and '{type(b).__name__}'.\")\n",
    "        sum = {}\n",
    "        for player in self.players:\n",
    "            sum[player.name] = a[player.name] + b[player.name]\n",
    "        return sum\n",
    "\n",
    "    # Function for measuring a certain value.\n",
    "    def meas(self, values: dict, pr: float) -> dict[str, float]:\n",
    "        if type(values) != dict or type(pr) != float:\n",
    "            raise TypeError(f\"Inputs must be of type 'dict' and 'float', not '{type(values).__name__}' and '{type(pr).__name__}'.\")\n",
    "        measured = {}\n",
    "        for player in self.players:\n",
    "            measured[player.name] = values[player.name] * pr\n",
    "        return measured # Returns the expected value.\n",
    "\n",
    "    # Computing the total expected value from a policy sequence when starting at time t in state x.\n",
    "    def val(self, t: int, ps: dict[list[dict[str, str]]], x: str) -> dict[str, float]:\n",
    "        if t < 0 or type(t) != int:\n",
    "            raise ValueError(f\"Invalid time step: '{t}' (must be positive integer).\")\n",
    "        if type(ps) != dict:\n",
    "            raise TypeError(f\"Invalid policy list, must be dictionary.\")\n",
    "        if x not in self.states:\n",
    "            raise ValueError(f\"Invalid state: '{x}'\")\n",
    "        \n",
    "        values = {player.name: self.zero for player in self.players}\n",
    "        ys = {}\n",
    "        for player in self.players:\n",
    "            if len(ps[player.name]) == 0:\n",
    "                return values\n",
    "            ys[player.name] = ps[player.name][0][x]\n",
    "        m_next = self.nextFunc(t, x, ys)\n",
    "        for x_prim, pr in m_next.items():\n",
    "            new_vals = self.meas(\n",
    "                self.add(\n",
    "                    self.reward(t, x, ys, x_prim), \n",
    "                    self.val(t+1, {p: ps[p][1:] for p in ps}, x_prim)\n",
    "                    ),\n",
    "                    pr)\n",
    "            for player in self.players:\n",
    "                values[player.name] += new_vals[player.name]\n",
    "\n",
    "        return values\n",
    "    \n",
    "    def bestExt(self, t: int, ps_tail: dict[list[dict[str, str]]]) -> dict[dict[str, str]]:\n",
    "        policy = {}\n",
    "\n",
    "        for player in self.players:\n",
    "            policy[player.name] = {}\n",
    "\n",
    "        for state in self.states:\n",
    "            # Generate list of list of player actions in current state.\n",
    "            all_actions = [player.actions(state) for player in self.players]  \n",
    "            \n",
    "            # Initialize best action tracking\n",
    "            best_values = {player.name: -np.inf for player in self.players}\n",
    "            best_actions = {player.name: None for player in self.players}\n",
    "\n",
    "            for action_combination in product(*all_actions):  # Generates all possible player action combinations.\n",
    "                ys = {}\n",
    "\n",
    "                for player, action in zip(self.players, action_combination):\n",
    "                    ys[player.name] = action  # Store the action for each player\n",
    "\n",
    "                ps_prim = copy.deepcopy(ps_tail)\n",
    "                for player in self.players:\n",
    "                    ps_prim[player.name].insert(0, {state: ys[player.name]})\n",
    "\n",
    "                # Compute the expected value for this action combination.\n",
    "                value = self.val(t, ps_prim, state)\n",
    "\n",
    "                # Update best action for each player based on their individual expected value\n",
    "                for player in self.players:\n",
    "                    if value[player.name] > best_values[player.name]:\n",
    "                        best_values[player.name] = value[player.name]\n",
    "                        best_actions[player.name] = ys[player.name]  # Now ys[player.name] is correctly populated\n",
    "\n",
    "            # Assign the best action found for this state.\n",
    "            for player in self.players:\n",
    "                policy[player.name][state] = best_actions[player.name]\n",
    "\n",
    "        return policy  \n",
    "\n",
    "    # Builds an optimal policy sequence by recursively adding the best extension (starting from the end).\n",
    "    def bi(self, t: int, n: int) -> list[dict[str, str]]:\n",
    "        if n == 0:\n",
    "            base = {player.name: [] for player in self.players}\n",
    "            return base\n",
    "        else:\n",
    "            ps_tail = self.bi(t + 1, n - 1)\n",
    "            p = self.bestExt(t, ps_tail)\n",
    "            for player in self.players:\n",
    "                ps_tail[player.name].insert(0, p[player.name])\n",
    "            return ps_tail\n",
    "\n",
    "    # For a given time step, state and decision horizon, returns the optimal action and the\n",
    "    # expected value of the sequence it starts (assuming the rest of the sequence is optimal).\n",
    "    def best(self, t: int, n: int, x: str) -> str:\n",
    "        if n <= 0:\n",
    "            raise ValueError(\"The horizon must be greater than zero!\")\n",
    "        ps = self.bi(t + 1, n - 1)\n",
    "        p = self.bestExt(t, ps)\n",
    "        b = {}\n",
    "        for player in self.players:\n",
    "            ps[player.name].insert(0, p[player.name])\n",
    "            b[player.name] = p[player.name][x]\n",
    "        vb = self.val(t, ps, x)\n",
    "        return f\"Horizon, best, value : {n}, {b}, {vb}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actions chosen: {'Player1': 'Confess', 'Player2': 'Confess'}\n",
      "Outcome distribution: {'CC': 1.0, 'CR': 0.0, 'RC': 0.0, 'RR': 0.0}\n",
      "Rewards: {'Player1': 1, 'Player2': 1}\n",
      "Values from ps1: {'Player1': 2.0, 'Player2': 2.0}\n",
      "Values from ps1: {'Player1': 6.0, 'Player2': 6.0}\n",
      "Best extension:\n",
      "           Start       CC       CR       RC       RR\n",
      "Player1  Confess  Confess  Confess  Confess  Confess\n",
      "Player2  Confess  Confess  Confess  Confess  Confess\n",
      "Best: Horizon, best, value : 1, {'Player1': 'Confess', 'Player2': 'Confess'}, {'Player1': 1.0, 'Player2': 1.0}\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    # Define probabilities.\n",
    "    probs = {\"pC_Confess\": 1.0, \"pR_Confess\": 0.0, \"pC_Refuse\": 0.0, \"pR_Refuse\": 1.0}\n",
    "    states = [\"Start\", \"CC\", \"CR\", \"RC\", \"RR\"]\n",
    "    def actions(x):\n",
    "        if x in states:\n",
    "            return [\"Confess\", \"Refuse\"]\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid State: '{x}'.\")\n",
    "    \n",
    "    # Create two players.\n",
    "    player1 = Player(\"Player1\", probs, states, actions)\n",
    "    player2 = Player(\"Player2\", probs, states, actions)\n",
    "    players = [player1, player2]\n",
    "    \n",
    "    # Create the game instance.\n",
    "    game = Game(players, states)\n",
    "    \n",
    "    # PLACEHOLDER.\n",
    "    actions_chosen = {}\n",
    "    for player in players:\n",
    "        action = player.choose_action(\"Start\")\n",
    "        actions_chosen[player.name] = action\n",
    "    print(\"Actions chosen:\", actions_chosen)\n",
    "    \n",
    "    # Determine the outcome of the transition.\n",
    "    outcome_distribution = game.nextFunc(0, game.current_state, actions_chosen)\n",
    "    print(\"Outcome distribution:\", outcome_distribution)\n",
    "    \n",
    "    # Compute rewards.\n",
    "    rewards = game.reward(0, \"Start\", actions_chosen, \"CC\")\n",
    "    print(\"Rewards:\", rewards)\n",
    "\n",
    "    ps1 = {\n",
    "        \"Player1\": [{\"Start\": \"Confess\", \"CC\": \"Confess\", \"CR\": \"Confess\", \"RC\": \"Confess\", \"RR\": \"Confess\"}, \n",
    "                    {\"Start\": \"Confess\", \"CC\": \"Confess\", \"CR\": \"Confess\", \"RC\": \"Confess\", \"RR\": \"Confess\"}], \n",
    "        \"Player2\": [{\"Start\": \"Confess\", \"CC\": \"Confess\", \"CR\": \"Confess\", \"RC\": \"Confess\", \"RR\": \"Confess\"}, \n",
    "                    {\"Start\": \"Confess\", \"CC\": \"Confess\", \"CR\": \"Confess\", \"RC\": \"Confess\", \"RR\": \"Confess\"}]\n",
    "        }\n",
    "    \n",
    "    ps2 = {\n",
    "        \"Player1\": [{\"Start\": \"Refuse\", \"CC\": \"Refuse\", \"CR\": \"Refuse\", \"RC\": \"Refuse\", \"RR\": \"Refuse\"}, \n",
    "                    {\"Start\": \"Refuse\", \"CC\": \"Refuse\", \"CR\": \"Refuse\", \"RC\": \"Refuse\", \"RR\": \"Refuse\"}], \n",
    "        \"Player2\": [{\"Start\": \"Refuse\", \"CC\": \"Refuse\", \"CR\": \"Refuse\", \"RC\": \"Refuse\", \"RR\": \"Refuse\"}, \n",
    "                    {\"Start\": \"Refuse\", \"CC\": \"Refuse\", \"CR\": \"Refuse\", \"RC\": \"Refuse\", \"RR\": \"Refuse\"}]\n",
    "        }\n",
    "    \n",
    "    ps3 = {\n",
    "        \"Player1\": [],\n",
    "        \"Player2\": []\n",
    "    }\n",
    "\n",
    "    values_ps1 = game.val(0, ps1, \"CC\")\n",
    "    print(\"Values from ps1:\", values_ps1)\n",
    "\n",
    "    values_ps2 = game.val(0, ps2, \"CC\")\n",
    "    print(\"Values from ps1:\", values_ps2)\n",
    "\n",
    "    best_extension = game.bestExt(0, ps3)\n",
    "    # Convert dictionary to DataFrame\n",
    "    df = pd.DataFrame.from_dict(best_extension, orient='index')\n",
    "    print(\"Best extension:\")\n",
    "    print(df)\n",
    "\n",
    "    # bi = game.bi(0, 3)\n",
    "    # print(\"Backwards induction:\", bi)\n",
    "    \n",
    "    best = game.best(0, 1, \"CC\")\n",
    "    print(\"Best:\", best)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
